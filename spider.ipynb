{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python爬虫 Urllib的高级用法\n",
    "\n",
    "## 1.设置Headers\n",
    "\n",
    "如果识别有问题，那么站点根本不会响应，所以为了完全模拟浏览器的工作，我们需要设置一些Headers 的属性。\n",
    "\n",
    "首先，打开我们的浏览器，调试浏览器F12，我用的是Chrome，打开网络监听，示意如下，比如知乎，点登录之后，我们会发现登陆之后界面都变化 了，出现一个新的界面，实质上这个页面包含了许许多多的内容，这些内容也不是一次性就加载完成的，实质上是执行了好多次请求，一般是首先请求HTML文 件，然后加载JS，CSS 等等，经过多次请求之后，网页的骨架和肌肉全了，整个网页的效果也就出来了。\n",
    "\n",
    "拆分这些请求，我们只看一第一个请求，你可以看到，有个Request URL，还有headers，下面便是response，图片显示得不全，小伙伴们可以亲身实验一下。那么这个头中包含了许许多多是信息，有文件编码啦，压缩方式啦，请求的agent啦等等。\n",
    "\n",
    "其中，agent就是请求的身份，如果没有写入请求身份，那么服务器不一定会响应，所以可以在headers中设置agent,例如下面的例子，这个例子只是说明了怎样设置的headers，小伙伴们看一下设置格式就好。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import urllib\n",
    "#import urllib2\n",
    "\n",
    "#url = \"https://www.zhihu.com/#signin\"\n",
    "#user_agent = 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'\n",
    "#values = {'user_login' : 'wuyunxing@hotmail.com','user_password' : 'wyx83695519' } \n",
    "#headers = { 'User-Agent' : user_agent }  \n",
    "#data = urllib.urlencode(values)  \n",
    "#request = urllib2.Request(url, data, headers)  \n",
    "#response = urllib2.urlopen(request)  \n",
    "#page = response.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样，我们设置了一个headers，在构建request时传入，在请求时，就加入了headers传送，服务器若识别了是浏览器发来的请求，就会得到响应。\n",
    "\n",
    "另外，我们还有对付”反盗链”的方式，对付防盗链，服务器会识别headers中的referer是不是它自己，如果不是，有的服务器不会响应，所以我们还可以在headers中加入referer\n",
    "\n",
    "例如我们可以构建下面的headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers = { 'User-Agent' : 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'  ,\n",
    "                        'Referer':'http://www.zhihu.com/articles' }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同上面的方法，在传送请求时把headers传入Request参数里，这样就能应付防盗链了。\n",
    "\n",
    "另外headers的一些属性，下面的需要特别注意一下：\n",
    "\n",
    "> User-Agent : 有些服务器或 Proxy 会通过该值来判断是否是浏览器发出的请求\n",
    "\n",
    "> Content-Type : 在使用 REST 接口时，服务器会检查该值，用来确定 HTTP Body 中的内容该怎样解析。\n",
    "\n",
    "> application/xml ： 在 XML RPC，如 RESTful/SOAP 调用时使用\n",
    "\n",
    "> application/json ： 在 JSON RPC 调用时使用\n",
    "\n",
    "> application/x-www-form-urlencoded ： 浏览器提交 Web 表单时使用在使用服务器提供的 RESTful 或 SOAP 服务时， Content-Type 设置错误会导致服务器拒绝服务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他的有必要的可以审查浏览器的headers内容，在构建时写入同样的数据即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Proxy(代理)的设置\n",
    "\n",
    "urllib2 默认会使用环境变量 http_proxy 来设置 HTTP Proxy。假如一个网站它会检测某一段时间某个IP 的访问次数，如果访问次数过多，它会禁止你的访问。所以你可以设置一些代理服务器来帮助你做工作，每隔一段时间换一个代理，网站君都不知道是谁在捣鬼了，这酸爽！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import urllib2\n",
    "#enable_proxy = True\n",
    "#proxy_handler = urllib2.ProxyHandler({\"http\" : 'http://some-proxy.com:8080'})\n",
    "#null_proxy_handler = urllib2.ProxyHandler({})\n",
    "#if enable_proxy:\n",
    "#    opener = urllib2.build_opener(proxy_handler)\n",
    "#else:\n",
    "#    opener = urllib2.build_opener(null_proxy_handler)\n",
    "#urllib2.install_opener(opener)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Timeout 设置\n",
    "\n",
    "上一节已经说过urlopen方法了，第三个参数就是timeout的设置，可以设置等待多久超时，为了解决一些网站实在响应过慢而造成的影响。\n",
    "\n",
    "例如下面的代码,如果第二个参数data为空那么要特别指定是timeout是多少，写明形参，如果data已经传入，则不必声明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2\n",
    "response = urllib2.urlopen('http://www.baidu.com', timeout=10)\n",
    "\n",
    "import urllib2\n",
    "response = urllib2.urlopen('http://www.baidu.com',data, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.使用 HTTP 的 PUT 和 DELETE 方法\n",
    "\n",
    "http协议有六种请求方法，get,head,put,delete,post,options，我们有时候需要用到PUT方式或者DELETE方式请求。\n",
    "\n",
    "> PUT：这个方法比较少见。HTML表单也不支持这个。本质上来讲， PUT和POST极为相似，都是向服务器发送数据，但它们之间有一个重要区别，PUT通常指定了资源的存放位置，而POST则没有，POST的数据存放位置由服务器自己决定。\n",
    "\n",
    "> DELETE：删除某一个资源。基本上这个也很少见，不过还是有一些地方比如amazon的S3云服务里面就用的这个方法来删除资源。\n",
    "\n",
    "如果要使用 HTTP PUT 和 DELETE ，只能使用比较低层的 httplib 库。虽然如此，我们还是能通过下面的方式，使 urllib2 能够发出 PUT 或DELETE 的请求，不过用的次数的确是少，在这里提一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import urllib2\n",
    "#request = urllib2.Request(uri, data=data)\n",
    "#request.get_method = lambda: 'PUT' # or 'DELETE'\n",
    "#response = urllib2.urlopen(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.使用DebugLog\n",
    "\n",
    "可以通过下面的方法把 Debug Log 打开，这样收发包的内容就会在屏幕上打印出来，方便调试，这个也不太常用，仅提一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "send: 'GET / HTTP/1.1\\r\\nAccept-Encoding: identity\\r\\nHost: www.baidu.com\\r\\nConnection: close\\r\\nUser-Agent: Python-urllib/2.7\\r\\n\\r\\n'\n",
      "reply: 'HTTP/1.1 200 OK\\r\\n'\n",
      "header: Date: Thu, 19 May 2016 16:43:44 GMT\r\n",
      "header: Content-Type: text/html; charset=utf-8\r\n",
      "header: Transfer-Encoding: chunked\r\n",
      "header: Connection: Close\r\n",
      "header: Vary: Accept-Encoding\r\n",
      "header: Set-Cookie: BAIDUID=25F5064D8020D819E409EDA2E500F1DD:FG=1; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.com\r\n",
      "header: Set-Cookie: BIDUPSID=25F5064D8020D819E409EDA2E500F1DD; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.com\r\n",
      "header: Set-Cookie: PSTM=1463676224; expires=Thu, 31-Dec-37 23:55:55 GMT; max-age=2147483647; path=/; domain=.baidu.com\r\n",
      "header: Set-Cookie: BDSVRTM=0; path=/\r\n",
      "header: Set-Cookie: BD_HOME=0; path=/\r\n",
      "header: Set-Cookie: H_PS_PSSID=19289_1434_18280_19558_19860_15068_11924; path=/; domain=.baidu.com\r\n",
      "header: P3P: CP=\" OTI DSP COR IVA OUR IND COM \"\r\n",
      "header: Cache-Control: private\r\n",
      "header: Cxy_all: baidu+580fffb383208837db81a417de896503\r\n",
      "header: Expires: Thu, 19 May 2016 16:43:28 GMT\r\n",
      "header: X-Powered-By: HPHP\r\n",
      "header: Server: BWS/1.1\r\n",
      "header: X-UA-Compatible: IE=Edge,chrome=1\r\n",
      "header: BDPAGETYPE: 1\r\n",
      "header: BDQID: 0xb8e41e8800025350\r\n",
      "header: BDUSERID: 0\r\n"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "httpHandler = urllib2.HTTPHandler(debuglevel=1)\n",
    "httpsHandler = urllib2.HTTPSHandler(debuglevel=1)\n",
    "opener = urllib2.build_opener(httpHandler, httpsHandler)\n",
    "urllib2.install_opener(opener)\n",
    "response = urllib2.urlopen('http://www.baidu.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上便是一部分高级特性，前三个是重要内容，在后面，还有cookies的设置还有异常的处理，小伙伴们加油！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python爬虫 URLError异常处理\n",
    "\n",
    "## 1.URLError\n",
    "\n",
    "首先解释下URLError可能产生的原因:\n",
    "\n",
    "* 网络无连接，即本机无法上网\n",
    "* 连接不到指定的服务器\n",
    "* 服务器不存在\n",
    "\n",
    "在代码中，我们需要用try-except语句来包围并捕获相应的异常。下面是一个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 8] nodename nor servname provided, or not known\n"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    "\n",
    "requset = urllib2.Request('http://www.yufytf.com')\n",
    "\n",
    "try:\n",
    "    urllib2.urlopen(requset)\n",
    "except urllib2.URLError, e:\n",
    "    print e.reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.HTTPError\n",
    "\n",
    "HTTPError是URLError的子类，在你利用urlopen方法发出一个请求时，服务器上都会对应一个应答对象response，其中它包含一个数字”状态码”。举个例子，假如response是一个”重定向”，需定位到别的地址获取文档，urllib2将对此进行处理。\n",
    "\n",
    "其他不能处理的，urlopen会产生一个HTTPError，对应相应的状态吗，HTTP状态码表示HTTP协议所返回的响应的状态。下面将状态码归结如下：\n",
    "\n",
    "> 100：继续 客户端应当继续发送请求。客户端应当继续发送请求的剩余部分，或者如果请求已经完成，忽略这个响应。\n",
    "\n",
    "> 101： 转换协议 在发送完这个响应最后的空行后，服务器将会切换到在Upgrade 消息头中定义的那些协议。只有在切换新的协议更有好处的时候才应该采取类似措施。\n",
    "\n",
    "> 102：继续处理 由WebDAV（RFC 2518）扩展的状态码，代表处理将被继续执行。\n",
    "\n",
    "> 200：请求成功 处理方式：获得响应的内容，进行处理\n",
    "\n",
    "> 201：请求完成，结果是创建了新资源。新创建资源的URI可在响应的实体中得到 处理方式：爬虫中不会遇到\n",
    "\n",
    "> 202：请求被接受，但处理尚未完成 处理方式：阻塞等待\n",
    "\n",
    "> 204：服务器端已经实现了请求，但是没有返回新的信 息。如果客户是用户代理，则无须为此更新自身的文档视图。 处理方式：丢弃\n",
    "\n",
    "> 300：该状态码不被HTTP/1.0的应用程序直接使用， 只是作为3XX类型回应的默认解释。存在多个可用的被请求资源。 处理方式：若程序中能够处理，则进行进一步处理，如果程序中不能处理，则丢弃\n",
    "\n",
    "> 301：请求到的资源都会分配一个永久的URL，这样就可以在将来通过该URL来访问此资源 处理方式：重定向到分配的URL\n",
    "\n",
    "> 302：请求到的资源在一个不同的URL处临时保存 处理方式：重定向到临时的URL\n",
    "\n",
    "> 304：请求的资源未更新 处理方式：丢弃\n",
    "\n",
    "> 400：非法请求 处理方式：丢弃\n",
    "\n",
    "> 401：未授权 处理方式：丢弃\n",
    "\n",
    "> 403：禁止 处理方式：丢弃\n",
    "\n",
    "> 404：没有找到 处理方式：丢弃\n",
    "\n",
    "> 500：服务器内部错误 服务器遇到了一个未曾预料的状况，导致了它无法完成对请求的处理。一般来说，这个问题都会在服务器端的源代码出现错误时出现。\n",
    "\n",
    "> 501：服务器无法识别 服务器不支持当前请求所需要的某个功能。当服务器无法识别请求的方法，并且无法支持其对任何资源的请求。\n",
    "\n",
    "> 502：错误网关 作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应。\n",
    "\n",
    "> 503：服务出错 由于临时的服务器维护或者过载，服务器当前无法处理请求。这个状况是临时的，并且将在一段时间以后恢复。\n",
    "\n",
    "HTTPError实例产生后会有一个code属性，这就是是服务器发送的相关错误号。\n",
    "因为urllib2可以为你处理重定向，也就是3开头的代号可以被处理，并且100-299范围的号码指示成功，所以你只能看到400-599的错误号码。\n",
    "\n",
    "下面我们写一个例子来感受一下，捕获的异常是HTTPError，它会带有一个code属性，就是错误代号，另外我们又打印了reason属性，这是它的父类URLError的属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "send: 'GET /cqcre HTTP/1.1\\r\\nAccept-Encoding: identity\\r\\nHost: blog.csdn.net\\r\\nConnection: close\\r\\nUser-Agent: Python-urllib/2.7\\r\\n\\r\\n'\n",
      "reply: 'HTTP/1.1 403 Forbidden\\r\\n'\n",
      "header: Server: openresty\r\n",
      "header: Date: Thu, 19 May 2016 16:53:38 GMT\r\n",
      "header: Content-Type: text/html; charset=utf-8\r\n",
      "header: Content-Length: 162\r\n",
      "header: Connection: close\r\n",
      "403\n",
      "Forbidden\n"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    " \n",
    "req = urllib2.Request('http://blog.csdn.net/cqcre')\n",
    "try:\n",
    "    urllib2.urlopen(req)\n",
    "except urllib2.HTTPError, e:\n",
    "    print e.code\n",
    "    print e.reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "错误代号是403，错误原因是Forbidden，说明服务器禁止访问。\n",
    "\n",
    "我们知道，HTTPError的父类是URLError，根据编程经验，父类的异常应当写到子类异常的后面，如果子类捕获不到，那么可以捕获父类的异常，所以上述的代码可以这么改写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "send: 'GET /cqcre HTTP/1.1\\r\\nAccept-Encoding: identity\\r\\nHost: blog.csdn.net\\r\\nConnection: close\\r\\nUser-Agent: Python-urllib/2.7\\r\\n\\r\\n'\n",
      "reply: 'HTTP/1.1 403 Forbidden\\r\\n'\n",
      "header: Server: openresty\r\n",
      "header: Date: Thu, 19 May 2016 16:54:30 GMT\r\n",
      "header: Content-Type: text/html; charset=utf-8\r\n",
      "header: Content-Length: 162\r\n",
      "header: Connection: close\r\n",
      "403\n"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    " \n",
    "req = urllib2.Request('http://blog.csdn.net/cqcre')\n",
    "try:\n",
    "    urllib2.urlopen(req)\n",
    "except urllib2.HTTPError, e:\n",
    "    print e.code\n",
    "except urllib2.URLError, e:\n",
    "    print e.reason\n",
    "else:\n",
    "    print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果捕获到了HTTPError，则输出code，不会再处理URLError异常。如果发生的不是HTTPError，则会去捕获URLError异常，输出错误原因。\n",
    "\n",
    "另外还可以加入 hasattr属性提前对属性进行判断，代码改写如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "send: 'GET /cqcre HTTP/1.1\\r\\nAccept-Encoding: identity\\r\\nHost: blog.csdn.net\\r\\nConnection: close\\r\\nUser-Agent: Python-urllib/2.7\\r\\n\\r\\n'\n",
      "reply: 'HTTP/1.1 403 Forbidden\\r\\n'\n",
      "header: Server: openresty\r\n",
      "header: Date: Thu, 19 May 2016 16:55:54 GMT\r\n",
      "header: Content-Type: text/html; charset=utf-8\r\n",
      "header: Content-Length: 162\r\n",
      "header: Connection: close\r\n",
      "403\n",
      "Forbidden\n"
     ]
    }
   ],
   "source": [
    "import urllib2\n",
    " \n",
    "req = urllib2.Request('http://blog.csdn.net/cqcre')\n",
    "\n",
    "try:\n",
    "    urllib2.urlopen(req)\n",
    "except urllib2.URLError, e:\n",
    "    if hasattr(e,\"code\"):\n",
    "        print e.code\n",
    "    if hasattr(e,\"reason\"):\n",
    "        print e.reason\n",
    "else:\n",
    "    print \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先对异常的属性进行判断，以免出现属性输出报错的现象。\n",
    "\n",
    "以上，就是对URLError和HTTPError的相关介绍，以及相应的错误处理办法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
